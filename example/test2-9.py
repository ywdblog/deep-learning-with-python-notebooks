# 1：概述
# 神经网络中，更新模型的权重这一步是最难的，对于模型的某个权重系数，你怎么知道这个系数应该增大还是减小，以及变化多少呢？有个比较好的方法就是梯度下降法，它的思想是：如果你想要减小某个函数的值，那么你就应该沿着这个函数的梯度的反方向走，这样你就会朝着函数值减小的方向走，直到函数值减小到最小值为止。
# 对于 z = x+y , 如果你知道y的变化方向，就可以推断出z的变化方向，用数字语言来讲，这些函数是可微的。即使这样的函数组合在一起，得到的函数也是可微的。
# 将模型系数映射到模型在数据批量上损失值的函数，也是可微的：模型系数的微小变化，将导致损失值发生可预测的微小变化。
# 可以用一个叫作梯度的数学运算符来描述：模型系数向不同方向移动时，损失值如何变化。计算出梯度，就可以利用它来更新系数，使损失值减小（在一次更新中全部完成，而不是一次更新一个系数）。

# 2：导数
# f(x+epsilon_x) = y + a * epsilon_x
# 由于函数是光滑的（函数曲线没有任何突变的角度），因此在某个点p附近，如果epsilon_x足够小，就可以将f近似地看作斜率为a的线性函数，这样epsilon_y就等于a * epsilon_x，显然只有在x足够接近p时，这个线性近似才有效
# 斜率a被称为f在p点的导数，如果a为负，那么说明x在p点附近的微增将导致f(x)减小 ；如果a为正，那么x的微增将导致f(x)增大。
# 对于每个可微函数f(x)（可微的意思是“可以被求导”，比如光滑的连续函数可以被求导），都存在一个导数函数f'(x)，将x的值映射为f在该点局部线性近似的斜率，比如f(x) = a * x的导数是f'(x) =a
# 优化的目的就是找到使f(x)最小化的x值，就此而言，函数求导是一个非常强大的工具。如果你想将x改变一个很小的因子epsilon_x，目的是将f(x)最小化，并且你知道f的导数，那么问题已经解决了：导数描述的就是，改变x后f(x)会如何变化。如果你想减小f(x)的值，那么只需将x沿着导数的反方向移动一小步。

# 3：张量运算的导数：梯度
# 导数这一概念可以应用于任意函数，只要函数所对应的表面是连续且光滑的。张量运算（或张量函数）的导数叫作梯度，梯度就是将导数这一概念推广到以张量为输入的函数
# 张量函数的梯度表示该函数所对应多维表面的曲率，它表示的是，当输入参数发生变化时，函数输出如何变化。

# y_pred = dot (W+x) #利用模型权重W和输入x计算预测值y_pred
# loss = dot (y_pred , y_true) #估算预测值的偏差
# 现在想利用梯度来更新W，以使loss_value变小。如何做到这一点呢？如果输入数据x和y_true保持不变，那么可以将前面的运算看作一个将模型权重W的值映射到损失值的函数。
# loss_value = f(W) # f描述的是当W变化时，损失值所形成的曲线（或高维表面）

# 假设W的当前值为W0。f在W0点的导数是一个张量grad(loss_value, W0)，其形状与W相同，每个元素grad(loss_value, W0)[i, j]表示当W0[i, j]发生变化时loss_value变化的方向和大小
# 张量grad(loss_value, W0)是函数f(W) = loss_value在W0处的梯度，也叫作“lossvalue相对于W在W0附近的梯度”。

# 偏导数
# 张量运算grad(f(W), W)以矩阵W为输入，它可以表示为标量函数grad_ij(f(W), w_ij)的组合，每个标量函数返回的是，loss_value =f(W)相对于W[i, j]的导数（假设W的其他所有元素都不变），grad_ij叫作f相对于W[i, j]的偏导数
# grad(loss_value, W0)具体代表什么呢？单变量函数f(x)的导数可以看作函数f曲线的斜率。同样，grad(loss_value, W0)可以看作表示loss_value = f(W)在W0附近最陡上升方向的张量，也表示这一上升方向的斜率。
# 每个偏导数表示f在某个方向上的斜率。对于一个函数f(x)，你可以通过将x沿着导数的反方向移动一小步来减小f(x)的值。
# 对于一个张量函数f(W)，你也可以通过将W沿着梯度的反方向移动来减小loss_value = f(W)，比如W1 = W0 - step * grad(f(W0), W0)，其中step是一个很小的比例因子。也就是说，沿着f最陡上升的反方向移动，直观上看可以移动到曲线上更低的位置。注意，比例因子step是必需的，因为grad(loss_value, W0)只是W0附近曲率的近似值，所以不能离W0太远。

# 4：随机梯度下降
# 给定一个可微函数，理论上可以用解析法找到它的最小值：函数的最小值就是导数为0的点，因此只需找到所有导数为0的点，然后比较函数在其中哪个点的取值最小。
# 将这一方法应用到神经网络中，就是将神经网络的损失函数看作一个可微函数，然后找到使损失函数最小化的权重值。这种方法叫作解析法，它的优点是，可以找到全局最小值，缺点是，计算量大，而且可能会陷入局部最小值。
# 计算损失相对于模型参数的梯度，这一步叫做反向传播。
# W -= step * grad(loss, W) # 将W沿着梯度的反方向移动一小步  W -= learning_rate * grad(loss, W)
# 学习率是一个调节梯度下降“速度”的标量因子

# mini-batch SGD vs true SGD  vs batch GD
# 小批量随机梯度下降SGD，是一种优化算法，它可以在训练神经网络时，快速找到最优解。它的基本思想是，将训练数据分成若干个小批量，然后对每个小批量进行梯度下降，每次迭代时，都随机选择一个小批量，然后计算梯度，最后将梯度应用于网络的权重，从而使网络的损失值减小。这种方法的优点是，可以利用GPU并行计算，从而大大加快训练速度。缺点是，由于每次迭代时，只使用了一个小批量的数据，所以梯度的方向并不总是准确的，因此可能会导致损失值震荡，甚至无法收敛。
# 真SGD：每次迭代时，都随机选择一个小批量，然后计算梯度，最后将梯度应用于网络的权重，从而使网络的损失值减小。
# 批量梯度下降：每次迭代时，都使用所有的训练数据，然后计算梯度，最后将梯度应用于网络的权重，从而使网络的损失值减小。

# SGD有很多变体，比如带动量的SGD，它的不同之处在于计算下一次权重更新时还要考虑上一次权重更新，而不仅是当前的梯度值，这些变量被称为优化器。
# 动量的概念被用于许多变体，解决了SGD的两个问题：收敛速度和局部最小值。
# 一个实现
 
'''
past_velocity=0
momentum = 0.1 
while loss>0.1:
    w,loss,gradient = get_current_parameters()
    welocity = past_velocity * momentum - learning_rate * gradient 
    w= w + momentum* welocity -  learning_rate * gradient 
    past_velocity = welocity
    update_paramter(w)
'''