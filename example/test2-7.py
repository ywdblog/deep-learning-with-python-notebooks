# -*- coding: utf-8 -*

# Tensor reshaping 张量变形

# 变形后，张量的元素个数与初始张量相同

x = np.array([[0., 1.],
             [2., 3.],
             [4., 5.]])

x = x.reshape((6, 1))

# 转置 transpose
x = np.zeros((300, 20))
x = np.transpose(x)


# 张量运算的几何解释

# 1： 张量加法表示将两个张量的元素逐个相加，x + y，相当于平移（不是变形）

# 2： 对一个二维向量旋转theta角，可以通过与一个2*2的矩阵进行点积运算，R =[[cos(theta),-sin*(theta)],[sin(theta,cos(theta)]]   x = R * x
# 缩放相当于在垂直方向和水平方向进行缩放，可以通过与一个2*2的矩阵进行点积运算，S = [[a,0],[0,b]]  x = S * x
# 线性变换，与任意矩阵A进行点积运算，x = A * x，相当于对x进行了一个线性变换，旋转和缩放都是线性变换的情况

# 3：仿射变换，与任意矩阵A进行点积运算，x = A * x + b，相当于对x进行了一个线性变换（旋转和缩放都是线性变换的情况，点积），再加上一个平移，这就是一个没有激活函数的Dense层的运算（仿射层）
# 这个Dense层的权重矩阵W是一个2*3的矩阵，偏置向量b是一个3维向量，所以Dense层的运算可以表示为，x = W * x + b

# 4：多次仿射变换，重复应用多次仿射变换相当于一次仿射变换，一个完全没有激活函数的Dense层组成的多层神经网络其实相当于一个Dense层，这样的“深度”神经网络其实就是一个线性模型
# 这就是需要激活函数（比如relu）的原因，激活函数可以打破线性模型的局限性，使得神经网络可以表示更加复杂的运算，进行非常复杂的非线性变换
